<!DOCTYPE html>
<html lang="en">

<head>

	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="description" content="">
	<meta name="author" content="">

	<title>K-CAP 2025</title>

	<!-- Bootstrap core CSS -->
	<link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

	<!-- Custom styles for this template -->
	<link href="css/scrolling-nav.css" rel="stylesheet">

	<link href="css/style.css" rel="stylesheet">

	<script src="vendor/jquery/jquery.min.js"></script>
</head>

<body id="page-top">

	<div id="nav-placeholder">

	</div>

	<script>
		$(function() {
			$("#nav-placeholder").load("nav.html");
			$("#sidecol").load("side.html");
		});
	</script>

	<section>
		<div class="container">
			<div class="row">
				<div class="col-lg-9 mx-auto">

					<div class="row">
						<div class="col-lg-12 mx-auto">
						  <h2>Call for Papers</h2>
							<p class="text-justify">Knowledge has played a fundamental role since the inception of Artificial Intelligence.  While the forms in which algorithms have leveraged knowledge have evolved over time, the need for efficient representations is ever more critical.  Recent advances in AI, such as the stunning performance of large language models,  have relied on the large amount of data available on the Web. There is growing agreement among researchers that it's important to look beyond the sheer volume of data and instead prioritize the development of methods that are accurate, precise, and efficient for capturing knowledge.</p>
							<p class="text-justify">The International Conference on Knowledge Capture (K-CAP) aims at bringing together an interdisciplinary group of researchers on a diverse set of topics with an interest in the development of knowledge capture. This involves the design and development of formalisms, methods, and tools that enable efficient and precise extraction and organization of knowledge from different sources and for various modalities of use, including, for example,  automated reasoning, machine learning, and human-machine teaming.</p>
							<p class="text-justify">To enable a vibrant and constructive discussion on scalable and precise knowledge capture, K-CAP 2025 calls for the participation of researchers from diverse areas of Artificial Intelligence, including, but not limited to,  knowledge representation and reasoning, knowledge acquisition, semantic web, intelligent user interfaces for knowledge acquisition and retrieval, query processing and question answering over heterogeneous knowledge bases, novel evaluation paradigms, problem-solving and reasoning, ethics and AI, explainability, neuro-symbolic AI, agents, information extraction from structured or unstructured data, machine learning and representation learning,  information enrichment and visualization, as well as researchers interested in cyber-infrastructures to foster the publication, retrieval, reuse, and integration of data.</p>


							<h4>Topics of interest</h4>
							<p class="text-justify">Areas of interest for submissions to K-CAP 25 include, but are not limited to, the following topics:</p>

							<p class="text-justify">
						  <ul>
							<li>Knowledge representation</li>
							<li>Knowledge acquisition</li>
							<li>Ethical aspects related to knowledge capture and acquisition</li>
							<li>Knowledge capture for supporting explainability and, vice-versa, leveraging explainability approaches for knowledge capture</li>
							<li>Intelligent user interfaces for knowledge acquisition and retrieval</li>
							<li>Innovative query processing and question answering over heterogeneous knowledge bases</li>
							<li>Novel evaluation paradigms for knowledge capture</li>
							<li>Problem-solving and reasoning</li>
							<li>The role of knowledge and knowledge capture in neuro-symbolic AI</li>
							<li>Compact knowledge representation, such as constraint networks and graphical models</li>
							<li>Knowledge capture in multi-agent systems</li>
							<li>Intersection of planning and knowledge capture</li>
							<li>Information extraction from text</li>
							<li>Metadata in knowledge capture processes</li>
							<li>Multi-modal knowledge capture from text, tables, images, video, or sound</li>
							<li>Machine learning and representation learning</li>
							<li>Information enrichment and visualization</li>
							<li>The role of language models in knowledge graph construction and representation</li>
							<li>Techniques for extracting structured knowledge from large-scale language models</li>
							<li>Applications of deep learning to knowledge representation and reasoning, such as graph neural networks and graph convolutional networks</li>
							<li>Advancements in representation learning and deep learning for knowledge capture</li>
							<li>Utilizing deep learning for information extraction from structured and unstructured data to improve knowledge capture</li>
						  
						  </ul>

							</p>
						  <h4>Submissions</h4>
							<p class="text-justify">The Thirteenth International Conference on Knowledge Capture, K-CAP 2025, features a full papers track for research papers, as well as tracks for short papers for visionary ideas. 
	                      <ul>
							<li><b>Full papers</b>,&nbsp;describing original research can be up to 8 pages long, <strong>including references and appendices</strong>. Full papers will appear in the conference proceedings and will be citable as K-CAP 2025 publications.</li>
									<li><b>Short papers</b>, which may describe (possibly preliminary or open-ended) research, applications (academic, industrial, or otherwise), and late-breaking results, can be up to 4 pages long, including references. Short papers will also appear in the conference proceedings and will be citable as K-CAP 2025 publications.</li>
									<li><b>Visionary papers</b>, which describe outrageous ideas or potentially preliminary but highly innovative research, can be up to 2 pages long, including references. Visionary papers will be presented as short talks in the conference program.</li>
									
						  </ul>
							</p>
						  <p class="text-justify">Authors of all paper types will present their work in plenary sessions during the conference. Presentation timing for the different types of papers will be announced once the program is crafted.</p>
							<p class="text-justify">K-CAP is not a double-blind conference, hence authors should list their names and affiliations on the submission. Please use the <a href="https://www.acm.org/publications/proceedings-template">ACM 2 column SIG
									Conference Proceedings template</a> for your submission. Submissions in HTML format are welcome, so long as authors of HTML submissions also provide a conversion of their submission to a PDF file that adheres to the required ACM template
								for proceedings. <b>Papers submitted to the main conference track should not have been published before in an archival venue, or currently be under review in an archival venue</b>. Authors are welcome to submit papers that have been
									published as a preprint on arXiv, or in a non-archival venue like a workshop.&nbsp;Authors are allowed to use generative models as a tool but not to include them as co-authors. The use of such tools must be acknowledged and properly documented in the papers. In particular, authors are encouraged to add a credit statement capturing the <a href="https://credit.niso.org/">contribution role</a> of authors and models.
							</p>
							<p class="text-justify">All submissions to K-CAP should be made through EasyChair: <a href="https://easychair.org/conferences/?conf=kcap2025">https://easychair.org/conferences/?conf=kcap2025</a>.
							  <!-- a href="https://easychair.org/conferences/?conf=kcap23">https://easychair.org/conferences/?conf=kcap23</a -->
							</p>

							<p class="text-justify">This year we will embrace the <a href="https://www.nature.com/articles/sdata201618">FAIR principles</a> by collecting structured metadata of the datasets, software, ontologies and methods generated by K-CAP submissions. Authors will be encouraged to add these resources in the
							  EasyChair submission form (together with a brief description). Accepted papers with resources will be highlighted in the main K-CAP conference page.</p>
							<p class="text-justify">An author should register per accepted SHORT or FULL paper in order to appear in the proceeding.&nbsp;</p>
<h4>Important Dates</h4>

						  <p class="text-justify">
								<ul>
									<li>Abstract submission: July 28, 2025</li>
									<li>Paper submission: August 4, 2025</li>
									<li>Author notification: September 22nd, 2025</li>
									<li>Camera ready version due: October 6th, 2025</li>
									<!-- <li>Camera ready version due: <del>September 29 2023</del> October 26th, 2023</li> -->
									<li>Conference dates: December 10-12, 2025</li>
								</ul>
								All deadlines are midnight AoE (Anywhere on Earth).
							</p>

						  <h4>ACM Publications Policies</h4>

							<p class="text-justify">
						    By submitting your article to an ACM Publication, you are hereby acknowledging that you and your co-authors are subject to all <a href="https://www.acm.org/publications/policies">ACM Publications Policies</a>, including ACM's new <a href="https://www.acm.org/publications/policies/research-involving-human-participants-and-subjects">Publications Policy on Research Involving Human Participants and Subjects</a>. Alleged violations of this policy or any ACM Publications Policy will be investigated by ACM and may result in a full retraction of your paper, in addition to other potential penalties, as per ACM Publications Policy.”&nbsp; </p>
							<p class="text-justify">
						  Please ensure that you and your co-authors <a href="https://orcid.org/register">obtain an ORCID ID</a>, so you can complete the publishing process for your accepted paper.  ACM has been involved in ORCID from the start and we have recently made a <a href="https://authors.acm.org/author-resources/orcid-faqs">commitment to collect ORCID IDs from all of our published authors.</a> The collection process has started and will roll out as a requirement throughout 2022.  We are committed to improve author discoverability, ensure proper attribution and contribute to ongoing community efforts around name normalization; your ORCID ID will help in these efforts.” </p>
						</div>
					</div>



				</div>

				<div class="col-lg-3 mx-auto">
					<div id="sidecol" />
				</div>

			</div>
		</div>
	</section>

	<!-- Footer -->
	<!--footer class="py-5 bg-dark">
		<div class="container">
			<p class="m-0 text-center text-white">Copyright &copy; Your Website 2020</p>
		</div>
		<!-- /.container -->
	<!--/footer-->

	<!-- Bootstrap core JavaScript -->
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

	<!-- Plugin JavaScript -->
	<script src="vendor/jquery-easing/jquery.easing.min.js"></script>

	<!-- Custom JavaScript for this theme -->
	<script src="js/scrolling-nav.js"></script>

</body>

</html>
